"""
Sahara Phase 3 â€” Voice & Safety
=================================
1. Offline STT via Vosk (Hindi/Marathi/English)
2. Distress detection via pitch + energy analysis (librosa)
3. Crisis protocol â€” auto-triggers if distress is high

Install:
  pip install vosk librosa sounddevice numpy flask
  
Download Vosk model (Hindi, works for Marathi too):
  wget https://alphacephei.com/vosk/models/vosk-model-small-hi-0.22.zip
  unzip vosk-model-small-hi-0.22.zip -d ./vosk_model
"""

import io, json, wave, numpy as np, librosa, sounddevice as sd
from vosk import Model, KaldiRecognizer
from flask import Flask, request, jsonify

# â”€â”€ Load Vosk STT model (offline, on-device) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ”„ Loading Vosk STT model...")
STT_MODEL   = Model("./vosk_model/vosk-model-small-hi-0.22")
SAMPLE_RATE = 16000
print("âœ… Vosk ready!")

# â”€â”€ 1. Speech-to-Text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def transcribe(audio_bytes: bytes) -> str:
    """Convert raw WAV audio bytes â†’ text using Vosk (fully offline)."""
    rec = KaldiRecognizer(STT_MODEL, SAMPLE_RATE)
    rec.AcceptWaveform(audio_bytes)
    result = json.loads(rec.FinalResult())
    return result.get("text", "").strip()


# â”€â”€ 2. Distress Detector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_distress(audio_bytes: bytes) -> dict:
    """
    Analyze voice audio for emotional distress signals.
    Uses 3 acoustic features as proxy:
      - Pitch variance   â†’ high = agitated / anxious
      - Speech rate      â†’ fast = panic / distress  
      - Energy (RMS)     â†’ loud/erratic = emotional dysregulation
    Returns: { score: 0-1, level: calm/mild/high, flags: [...] }
    """
    # Load audio from bytes
    audio, sr = librosa.load(io.BytesIO(audio_bytes), sr=SAMPLE_RATE, mono=True)

    flags = []
    score = 0.0

    # Pitch variance (F0)
    f0, _, _ = librosa.pyin(audio, fmin=80, fmax=400, sr=sr)
    f0_clean  = f0[~np.isnan(f0)]
    if len(f0_clean) > 10:
        pitch_var = float(np.std(f0_clean))
        if pitch_var > 60:
            flags.append("high_pitch_variance")
            score += 0.35

    # Energy / RMS
    rms      = librosa.feature.rms(y=audio)[0]
    rms_var  = float(np.std(rms))
    rms_mean = float(np.mean(rms))
    if rms_var > 0.02:
        flags.append("erratic_energy")
        score += 0.30
    if rms_mean > 0.08:
        flags.append("loud_speech")
        score += 0.15

    # Speech rate via zero-crossing rate (proxy)
    zcr      = librosa.feature.zero_crossing_rate(audio)[0]
    zcr_mean = float(np.mean(zcr))
    if zcr_mean > 0.15:
        flags.append("fast_speech_rate")
        score += 0.20

    score = round(min(score, 1.0), 2)
    level = "high" if score >= 0.6 else "mild" if score >= 0.3 else "calm"

    return {"score": score, "level": level, "flags": flags}


# â”€â”€ 3. Crisis keywords (EN + HI + MR) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRISIS_WORDS = [
    "suicide", "kill myself", "end my life", "want to die",
    "jeena nahi", "marna chahta", "jiv dyaycha", "sampvaycha", "nako aata"
]

CRISIS_RESPONSE = {
    "response": "ğŸ’™ Tum akele nahi ho. Please abhi inhe call karo:\n\n"
                "â€¢ iCall: 9152987821\n"
                "â€¢ Vandrevala: 1860-2662-345 (24/7)\n"
                "â€¢ SNEHI: 044-24640050\n\n"
                "Sahara tumhare saath hai. ğŸ™",
    "crisis": True
}

def is_crisis(text: str) -> bool:
    return any(kw in text.lower() for kw in CRISIS_WORDS)


# â”€â”€ 4. Flask endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = Flask(__name__)

@app.route("/voice", methods=["POST"])
def voice():
    """
    Accepts: multipart WAV audio file
    Returns: { transcript, distress, crisis, forward_to_llm }
    
    React Native sends audio as:
      const form = new FormData();
      form.append('audio', { uri, type: 'audio/wav', name: 'voice.wav' });
      fetch('http://127.0.0.1:5007/voice', { method: 'POST', body: form });
    """
    if "audio" not in request.files:
        return jsonify({"error": "audio file required"}), 400

    audio_bytes = request.files["audio"].read()

    # Step 1 â€” Transcribe
    transcript = transcribe(audio_bytes)
    if not transcript:
        return jsonify({"error": "Could not understand audio"}), 422

    # Step 2 â€” Crisis keyword check (always first)
    if is_crisis(transcript):
        return jsonify({**CRISIS_RESPONSE, "transcript": transcript, "distress": None})

    # Step 3 â€” Distress analysis
    distress = analyze_distress(audio_bytes)

    # Step 4 â€” If high distress, override with calming response before LLM
    if distress["level"] == "high":
        return jsonify({
            "transcript":      transcript,
            "distress":        distress,
            "crisis":          False,
            "calming_prompt":  True,
            "response":        "ğŸŒ¬ï¸ Main sun raha/rahi hoon... Pehle ek gehri saans lo. "
                               "Saath mein... andar... aur bahar. ğŸ’™\n\n"
                               "Ab batao, kya chal raha hai?",
            "forward_to_llm":  False,   # pause, calm first
        })

    # Step 5 â€” Safe to forward transcript to Phase 2 LLM
    return jsonify({
        "transcript":     transcript,
        "distress":       distress,
        "crisis":         False,
        "forward_to_llm": True,         # React Native calls /chat with this transcript
    })


if __name__ == "__main__":
    print("ğŸŒ¿ Sahara Phase 3 voice bridge â†’ http://127.0.0.1:5007")
    app.run(port=5007)